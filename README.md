# Introduction
This repository contains various artifacts, such as source code, experimental results, and other materials, that supplement our work on the Intransitive Composite Object Normal Form (iCONF).\
&nbsp;&nbsp;&nbsp;&nbsp;Foremost, the repository contains an implementation of the lossless, dependency-preserving decomposition algorithm that minimizes the key number of subschemata in BCNF and the non-key FD number of subschemata in 3NF. Variant A1: <kbd>src/nf/iCONFOpt_minf_maxk.java</kbd> breaks further ties between redundant 3NF subschemata (in case they have the same number of non-key FDs) by prioritizing those with a higher number of minimal keys, while variant A2: <kbd>src/nf/iCONF.java</kbd>) only breaks ties using the number of non-key FDs. We have also implemented an additional three algorithms that allow us to compare our algorithms with the previous state-of-the-art (SOTA) algorithm CONF: A3(<kbd>src/nf/CONF.java</kbd>), as well as BC-Cover: A4(<kbd>src/nf/DecompAlg2.java</kbd>) and Synthesis: A5(<kbd>src/nf/DecompAlg4.java</kbd>). We have also included the code of implementations for other experiments: <kbd>src/exp/</kbd> and <kbd>src/util/</kbd>. For all experimental results, logs and some sql scripts are included, too (<kbd>Artifact/Experiments/</kbd>). In the following sections, we describe how our experiments can be reproduced. 
# Preliminaries: Getting databases ready for experiments
> 1. Import 12 datasets as SQL databases
>> We have used MySQL 8.0.29 as database workbench. Firstly, please create a database. Afterwards, import the [12 datasets](https://hpi.de/naumann/projects/repeatability/data-profiling/fds.html) as MySQL databases by setting column names as 0,1,...,n-1 where n is the number of columns in a given dataset. In addition, please create a column named "id" as an auto_increment attribute for each table that will facilitate us to remove updated tuples quickly.
> 2. Import TPC-H benchmark
>> Please visit the [website](https://relational.fit.cvut.cz/dataset/TPCH) and export the TPC-H database as an .sql file. Then, please import the file in your own local MySQL workbench. Under <kbd>Artifact/Experiments/TPCH/</kbd> we have included all 22 official SQL queries and refresh functions to be used in our experiments.
>3. Functional dependencies (FDs)
>> For each of the 12 datasets, the atomic closure for the set of FDs that hold on a dataset are given as separate json files in <kbd>Artifact/FD/</kbd>. For the TPC-H benchmark, the FDs(including keys) are in <kbd>Artifact/Experiments/TPCH/TPCH_schemata(1st exp).txt or TPCH_schemata(2nd exp).txt</kbd>.
>4. JDK & JDBC
>> Our code was developed and run in JAVA with version 17.0.7. At the same time, we used JDBC (version 8.0.26) as a connector to access MySQL databases.
# Experiments
In line with our paper, our experiments are organized into four sections. For each of them, you can run different code/scripts:
>1. Mini Study
>> In this part, we do a mini-study to show our initial research motivation in the introduction. Using the same FDs as input, iCONF we currently proposed and CONF (SOTA algorithm) output two different decompositions. For each decomposition, we insert the same records on each subschema of the decomposition. The mini-study shows that the decomposition from iCONF saves much more time than that of CONF. To reproduce the mini-study experiment, you can set up some parameters and run the code at <kbd>src/exp/SyntheticExpForCaseStudy.java</kbd>.
>2. Why do we parameterize normalization?
>> In this experiment, We ran the entire TPC-H benchmark (scaling factor 0.1) with the 22 queries, 7 refresh and 3 insert (adding 1k, 2k, and 3k of records) operations after declaring 1 to 5 minimal keys as UNIQUE constraints and enforcing 1 to 5 non-key FDs by triggers on each table. Through the workload experiments, our TPC-H study emphasizes the need for reducing the tremendous overhead caused by non-key FDs during updates. To reproduce the experiment, you can run the code at <kbd>src/exp/TPCHWorkloadExp.java</kbd>.
>3. How good are our algorithms?
>> We show our proposed algorithm's efficiency in this section. The experiments illustrated what our algorithms achieve over SOTA, particularly in terms of reducing the number of non-key FDs in critical schemata. It is very easy to get the different decompositions at the same time by running the code, which is located in <kbd>src/exp/DecompExp.java</kbd>. Or you can separately run each decomposition algorithm in <kbd>src/nf/</kbd>.
>4. How much overhead do we save?
>> Finally, we will illustrate how much overhead our proposed algorithm will save. For that purpose, we insert 10k, 20k, and 30k of records into hepatitis, abalone, ncvoter, lineitem, and weather. These insertions are done for the projections of these records onto the output schemata of our decompositions, resulting from iConf-f>k, iConf-f<k, iConf->kf, iConf-f, Conf, BC-Cover, and Synthesis. To reproduce the experiment, you can run the code at <kbd>src/exp/SubschemaPerfExp.java</kbd>.
